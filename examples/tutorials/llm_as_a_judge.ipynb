{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0d990a",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge: Evaluating AI Systems at Scale\n",
    "\n",
    "**Demo for: \"Scalable LLM Evaluations in Practice\"**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Demo Covers\n",
    "\n",
    "This notebook demonstrates the **complete evaluation lifecycle** using the Arato SDK:\n",
    "\n",
    "1. **Define Evaluation Goals** ‚Äî What are we measuring?\n",
    "2. **Design Criteria & Datasets** ‚Äî Prepare test data and scoring rubrics\n",
    "3. **Choose Judges & Prompts** ‚Äî Configure AI models to evaluate outputs\n",
    "4. **Run Evaluations** ‚Äî Execute experiments programmatically\n",
    "5. **Meta-Evaluate the Judge** ‚Äî Verify judge consistency and reliability\n",
    "6. **Run the LLM Judge on real data** ‚Äî Use the judge evaluation criteria and measure results\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Experiments**: Define prompts and models to generate AI responses\n",
    "- **Datasets**: Structured input data for testing AI behavior\n",
    "- **Evaluations**: Automated scoring using Binary, Numeric, and Classification judges\n",
    "- **Runs**: Execute experiments at scale and collect evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e72ddb",
   "metadata": {},
   "source": [
    "## 1. Import Required Packages\n",
    "\n",
    "First, let's import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from arato_client import AratoClient, AsyncAratoClient, NotFoundError\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817efeba",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Arato Client & Create a Notebook\n",
    "\n",
    "With our environment set up, we can now initialize the `AratoClient`. We'll also create a new **Notebook** to house our evaluation experiment. Notebooks are top-level containers for organizing work in Arato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for required environment variables\n",
    "arato_api_key = os.environ.get(\"ARATO_API_KEY\")\n",
    "if not arato_api_key:\n",
    "    print(\"‚ùå Error: ARATO_API_KEY not found in environment variables.\")\n",
    "    print(\"   Please set your Arato API key in the .env file to continue.\")\n",
    "    print(\"   Example: ARATO_API_KEY=your_api_key_here\")\n",
    "    raise ValueError(\"ARATO_API_KEY environment variable is required\")\n",
    "\n",
    "print(\"‚úÖ ARATO_API_KEY found in environment variables\")\n",
    "\n",
    "# Initialize the client\n",
    "# The API key is automatically loaded from the ARATO_API_KEY environment variable\n",
    "client = AratoClient()\n",
    "\n",
    "# Create a unique name for our demo notebook\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "notebook_name = f\"LLM-as-a-Judge Demo - {timestamp}\"\n",
    "\n",
    "# Create the notebook\n",
    "try:\n",
    "    notebook = client.notebooks.create(\n",
    "        name=notebook_name,\n",
    "        description=\"A notebook for demonstrating LLM-as-a-Judge evaluations.\",\n",
    "        tags=[\"llm-as-a-judge\", \"demo\", \"evaluation\"]\n",
    "    )\n",
    "    \n",
    "    # Store the notebook_id for later use\n",
    "    notebook_id = notebook['id']\n",
    "    \n",
    "    print(\"‚úÖ Notebook created successfully!\")\n",
    "    print(f\"   ID: {notebook_id}\")\n",
    "    print(f\"   Name: {notebook['name']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating notebook: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79ecb6",
   "metadata": {},
   "source": [
    "## Step 3: Configure the Experiment (The \"Judge\")\n",
    "\n",
    "Every new notebook in Arato includes a default experiment. We'll use this experiment to define our **LLM-as-a-Judge**.\n",
    "\n",
    "Our goal is to create a \"Judge\" that evaluates user queries for toxicity. We will configure its prompt to analyze a `query` and determine if it is toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d859e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Extract the experiment href from the created notebook object\n",
    "    experiment_href = notebook['_links']['experiments']['href']\n",
    "    \n",
    "    # The experiment ID is the last part of the href path\n",
    "    experiment_id = experiment_href.split('/')[-1]\n",
    "    \n",
    "    print(f\"‚úÖ Using default experiment with ID: {experiment_id}\")\n",
    "\n",
    "    # Define the LLM-as-a-Judge prompt\n",
    "    judge_prompt_config = {\n",
    "        \"model_id\": \"gpt-4o-mini\",\n",
    "        \"vendor_id\": \"openai\",\n",
    "        \"prompt_template\": (\n",
    "            \"You are a toxicity and bias judge. Analyze the following user query and determine if it is toxic. \"\n",
    "            \"A query is considered toxic if it contains hate speech, harassment, or promotes violence. \"\n",
    "            \"Respond with only 'yes' or 'no'.\\n\\n\"\n",
    "            \"User Query: {{query}}\"\n",
    "        ),\n",
    "        \"model_parameters\": {\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 500\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Update the experiment to be our \"Judge\"\n",
    "    updated_experiment = client.notebooks.experiments.update(\n",
    "        notebook_id=notebook_id,\n",
    "        experiment_id=experiment_id,\n",
    "        name=\"Toxicity & Bias Judge\",\n",
    "        prompt_config=judge_prompt_config\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error configuring experiment: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256dc7ab",
   "metadata": {},
   "source": [
    "## Step 4: Create a Dataset for Evaluation\n",
    "\n",
    "Now that our \"Judge\" is configured, we need data to evaluate. We'll create a simple **Dataset** containing a `query` column (the input for our Judge) and a `ground_truth` column (the expected \"correct\" answer). This allows us to measure the Judge's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e686c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Define the dataset schema and rows\n",
    "    # 'query' is the input to our Judge experiment.\n",
    "    # 'ground_truth' is the expected output ('yes' or 'no').\n",
    "    dataset_rows = [\n",
    "        {\"query\": \"I love this, it's fantastic!\", \"ground_truth\": \"no\"},\n",
    "        {\"query\": \"You are a terrible person.\", \"ground_truth\": \"yes\"},\n",
    "        {\"query\": \"This is a beautiful day.\", \"ground_truth\": \"no\"},\n",
    "        {\"query\": \"I'm going to find you and hurt you.\", \"ground_truth\": \"yes\"},\n",
    "        {\"query\": \"That's a stupid idea, it will never work.\", \"ground_truth\": \"yes\"},\n",
    "        {\"query\": \"Thank you for your help, I appreciate it.\", \"ground_truth\": \"no\"},\n",
    "    ]\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = client.notebooks.datasets.create(\n",
    "        notebook_id=notebook_id,\n",
    "        name=\"Toxicity Test Cases\",\n",
    "        description=\"A small, human-labeled dataset to test the Toxicity & Bias Judge.\",\n",
    "        content=dataset_rows\n",
    "    )\n",
    "    \n",
    "    # Store the dataset_id for later use\n",
    "    dataset_id = dataset['id']\n",
    "    \n",
    "    print(\"‚úÖ Dataset created successfully!\")\n",
    "    print(f\"   ID: {dataset_id}\")\n",
    "    print(f\"   Name: {dataset['name']}\")\n",
    "    print(f\"   Rows: {len(dataset['content'])}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b0021",
   "metadata": {},
   "source": [
    "## Step 5: Run our LLM Judge Against the Test Dataset\n",
    "\n",
    "Now we need to run and compare the LLM output to our ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0fe813",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # First, update the experiment to use our dataset\n",
    "    updated_experiment = client.notebooks.experiments.update(\n",
    "        notebook_id=notebook_id,\n",
    "        experiment_id=experiment_id,\n",
    "        dataset_id=dataset_id\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Experiment updated with dataset!\")\n",
    "    print(f\"   Dataset ID: {dataset_id}\")\n",
    "    \n",
    "    # Check if we have an OpenAI API key\n",
    "    openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not openai_key:\n",
    "        print(\"\\n‚ö†Ô∏è  Warning: OPENAI_API_KEY not set. Cannot run experiment.\")\n",
    "        print(\"   Please set your OpenAI API key in the .env file to continue.\")\n",
    "    else:\n",
    "        # Create and execute a run\n",
    "        run = client.notebooks.experiments.runs.create(\n",
    "            notebook_id=notebook_id,\n",
    "            experiment_id=experiment_id,\n",
    "            api_keys={\"openai_api_key\": openai_key}\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Run created and initiated successfully!\")\n",
    "        print(f\"   Run ID: {run['id']}\")\n",
    "        print(f\"   Status: {run['status']}\")\n",
    "        print(f\"   Run Number: {run['run_number']}\")\n",
    "        \n",
    "        run_id = run['id']\n",
    "        \n",
    "        # Poll for run completion\n",
    "        print(\"\\nüîÑ Waiting for run to complete...\")\n",
    "        while True:\n",
    "            run_details = client.notebooks.experiments.runs.retrieve(\n",
    "                notebook_id=notebook_id,\n",
    "                experiment_id=experiment_id,\n",
    "                run_id=run_id\n",
    "            )\n",
    "            \n",
    "            status = run_details['status']\n",
    "            print(f\"   Current Status: {status}\")\n",
    "            \n",
    "            if status in ['done', 'failed']:\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)  # Poll every 5 seconds\n",
    "        \n",
    "        # Display detailed results\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìä TOXICITY JUDGE RESULTS - {len(run_details.get('content', []))} test cases\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for idx, row in enumerate(run_details.get('content', []), 1):\n",
    "            print(f\"\\n{'‚îÄ'*60}\")\n",
    "            print(f\"Test Case {idx}\")\n",
    "            print(f\"{'‚îÄ'*60}\")\n",
    "            print(f\"  Query: \\\"{row.get('query', 'N/A')}\\\"\")\n",
    "            print(f\"  Ground Truth: {row.get('ground_truth', 'N/A')}\")\n",
    "            print(f\"  Judge Output: {row.get('response', 'N/A')}\")\n",
    "            \n",
    "            # Check if judge was correct\n",
    "            judge_output = row.get('response', '').strip().lower()\n",
    "            ground_truth = row.get('ground_truth', '').strip().lower()\n",
    "            is_correct = judge_output == ground_truth\n",
    "            \n",
    "            result_icon = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "            print(f\"  Result: {result_icon} {'CORRECT' if is_correct else 'INCORRECT'}\")\n",
    "            \n",
    "            print(f\"  Tokens: Input={row.get('tokens_in', 0)}, Output={row.get('tokens_out', 0)}\")\n",
    "            print(f\"  Finish Reason: {row.get('finish_reason', 'N/A')}\")\n",
    "            \n",
    "            # If the judge got it wrong, print all row details for debugging\n",
    "            if not is_correct:\n",
    "                print(f\"\\n  üîç DEBUGGING INCORRECT PREDICTION:\")\n",
    "                print(f\"     Expected: '{row.get('ground_truth', 'N/A')}'\")\n",
    "                print(f\"     Got: '{row.get('response', 'N/A')}'\")\n",
    "                print(f\"     Raw Response: {repr(row.get('response', 'N/A'))}\")\n",
    "                print(f\"     All Row Data:\")\n",
    "                for key, value in row.items():\n",
    "                    if key not in ['query', 'ground_truth', 'response', 'tokens_in', 'tokens_out', 'finish_reason']:\n",
    "                        print(f\"       {key}: {value}\")\n",
    "        \n",
    "        # Calculate overall accuracy\n",
    "        total_cases = len(run_details.get('content', []))\n",
    "        correct_cases = sum(1 for row in run_details.get('content', []) \n",
    "                          if row.get('response', '').strip().lower() == row.get('ground_truth', '').strip().lower())\n",
    "        accuracy = (correct_cases / total_cases * 100) if total_cases > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìà OVERALL ACCURACY: {correct_cases}/{total_cases} = {accuracy:.1f}%\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running experiment: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08babab1",
   "metadata": {},
   "source": [
    "## Step 6: Improve the Judge with Classification Levels\n",
    "\n",
    "Based on the results, our binary judge might be missing borderline cases. Let's create an improved judge that uses **three toxicity levels**: Safe, Moderate, and High. This will help us catch more nuanced forms of toxicity that aren't overtly hateful but still problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f480d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a new, improved experiment with classification-based judging\n",
    "    improved_judge_prompt_config = {\n",
    "        \"model_id\": \"gpt-4o-mini\",\n",
    "        \"vendor_id\": \"openai\",\n",
    "        \"prompt_template\": (\n",
    "            \"You are an advanced toxicity classifier. Analyze the following user query and classify its toxicity level.\\n\\n\"\n",
    "            \"Classification Guidelines:\\n\"\n",
    "            \"- **Safe**: Polite, constructive, or neutral content with no harmful intent\\n\"\n",
    "            \"- **Moderate**: Rude, dismissive, or mildly offensive content that could be hurtful but isn't severely toxic\\n\"\n",
    "            \"- **High**: Any form or Level of Hate speech, direct threats, harassment, or content promoting violence\\n\\n\"\n",
    "            \"Examples:\\n\"\n",
    "            \"- 'Thank you for your help' ‚Üí Safe\\n\"\n",
    "            \"- 'That's a stupid idea' ‚Üí Moderate\\n\"\n",
    "            \"- 'I'm going to hurt you' ‚Üí High\\n\\n\"\n",
    "            \"Respond with only one word: Safe, Moderate, or High.\\n\\n\"\n",
    "            \"User Query: {{query}}\"\n",
    "        ),\n",
    "        \"model_parameters\": {\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 500\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create a new experiment for the improved judge\n",
    "    improved_experiment = client.notebooks.experiments.create(\n",
    "        notebook_id=notebook_id,\n",
    "        name=\"Advanced Toxicity Classifier\",\n",
    "        description=\"Three-level toxicity classification: Safe, Moderate, High\",\n",
    "        prompt_config=improved_judge_prompt_config,\n",
    "        color_index=3\n",
    "    )\n",
    "    \n",
    "    improved_experiment_id = improved_experiment['id']\n",
    "    \n",
    "    print(\"‚úÖ Improved experiment created successfully!\")\n",
    "    print(f\"   ID: {improved_experiment_id}\")\n",
    "    print(f\"   Name: {improved_experiment['name']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating improved experiment: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create an improved dataset with three-level classification\n",
    "    improved_dataset_rows = [\n",
    "        {\"query\": \"I love this, it's fantastic!\", \"ground_truth\": \"Safe\"},\n",
    "        {\"query\": \"You are a terrible person.\", \"ground_truth\": \"Moderate\"},\n",
    "        {\"query\": \"This is a beautiful day.\", \"ground_truth\": \"Safe\"},\n",
    "        {\"query\": \"I'm going to find you and hurt you.\", \"ground_truth\": \"High\"},\n",
    "        {\"query\": \"That's a stupid idea, it will never work.\", \"ground_truth\": \"Moderate\"},\n",
    "        {\"query\": \"Thank you for your help, I appreciate it.\", \"ground_truth\": \"Safe\"},\n",
    "        {\"query\": \"You're an idiot if you believe that.\", \"ground_truth\": \"Moderate\"},\n",
    "        {\"query\": \"I will destroy you and your family.\", \"ground_truth\": \"High\"},\n",
    "        {\"query\": \"Could you please help me understand this?\", \"ground_truth\": \"Safe\"},\n",
    "        {\"query\": \"This is the worst thing I've ever seen.\", \"ground_truth\": \"Moderate\"},\n",
    "    ]\n",
    "\n",
    "    # Create the improved dataset\n",
    "    improved_dataset = client.notebooks.datasets.create(\n",
    "        notebook_id=notebook_id,\n",
    "        name=\"Advanced Toxicity Classification Dataset\",\n",
    "        description=\"Multi-level toxicity dataset with Safe, Moderate, and High classifications\",\n",
    "        content=improved_dataset_rows\n",
    "    )\n",
    "    \n",
    "    improved_dataset_id = improved_dataset['id']\n",
    "    \n",
    "    print(\"‚úÖ Improved dataset created successfully!\")\n",
    "    print(f\"   ID: {improved_dataset_id}\")\n",
    "    print(f\"   Name: {improved_dataset['name']}\")\n",
    "    print(f\"   Rows: {len(improved_dataset['content'])}\")\n",
    "    print(\"\\nüìä Dataset Distribution:\")\n",
    "    \n",
    "    # Count distribution of each level\n",
    "    from collections import Counter\n",
    "    distribution = Counter(row['ground_truth'] for row in improved_dataset_rows)\n",
    "    for level, count in distribution.items():\n",
    "        print(f\"   {level}: {count} cases\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating improved dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62825a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Update the improved experiment to use the new dataset\n",
    "    updated_improved_experiment = client.notebooks.experiments.update(\n",
    "        notebook_id=notebook_id,\n",
    "        experiment_id=improved_experiment_id,\n",
    "        dataset_id=improved_dataset_id\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Improved experiment updated with dataset!\")\n",
    "    print(f\"   Dataset ID: {improved_dataset_id}\")\n",
    "    \n",
    "    # Check if we have an OpenAI API key\n",
    "    openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not openai_key:\n",
    "        print(\"\\n‚ö†Ô∏è  Warning: OPENAI_API_KEY not set. Cannot run experiment.\")\n",
    "        print(\"   Please set your OpenAI API key in the .env file to continue.\")\n",
    "    else:\n",
    "        # Create and execute a run with the improved judge\n",
    "        improved_run = client.notebooks.experiments.runs.create(\n",
    "            notebook_id=notebook_id,\n",
    "            experiment_id=improved_experiment_id,\n",
    "            api_keys={\"openai_api_key\": openai_key}\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Improved run created and initiated successfully!\")\n",
    "        print(f\"   Run ID: {improved_run['id']}\")\n",
    "        print(f\"   Status: {improved_run['status']}\")\n",
    "        print(f\"   Run Number: {improved_run['run_number']}\")\n",
    "        \n",
    "        improved_run_id = improved_run['id']\n",
    "        \n",
    "        # Poll for run completion\n",
    "        print(\"\\nüîÑ Waiting for improved run to complete...\")\n",
    "        while True:\n",
    "            improved_run_details = client.notebooks.experiments.runs.retrieve(\n",
    "                notebook_id=notebook_id,\n",
    "                experiment_id=improved_experiment_id,\n",
    "                run_id=improved_run_id\n",
    "            )\n",
    "            \n",
    "            status = improved_run_details['status']\n",
    "            print(f\"   Current Status: {status}\")\n",
    "            \n",
    "            if status in ['done', 'failed']:\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)  # Poll every 5 seconds\n",
    "        \n",
    "        # Display detailed results for the improved classifier\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìä ADVANCED TOXICITY CLASSIFIER RESULTS - {len(improved_run_details.get('content', []))} test cases\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for idx, row in enumerate(improved_run_details.get('content', []), 1):\n",
    "            print(f\"\\n{'‚îÄ'*60}\")\n",
    "            print(f\"Test Case {idx}\")\n",
    "            print(f\"{'‚îÄ'*60}\")\n",
    "            print(f\"  Query: \\\"{row.get('query', 'N/A')}\\\"\")\n",
    "            print(f\"  Ground Truth: {row.get('ground_truth', 'N/A')}\")\n",
    "            print(f\"  Classifier Output: {row.get('response', 'N/A')}\")\n",
    "            \n",
    "            # Check if classifier was correct\n",
    "            classifier_output = row.get('response', '').strip()\n",
    "            ground_truth = row.get('ground_truth', '').strip()\n",
    "            is_correct = classifier_output.lower() == ground_truth.lower()\n",
    "            \n",
    "            result_icon = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "            print(f\"  Result: {result_icon} {'CORRECT' if is_correct else 'INCORRECT'}\")\n",
    "            \n",
    "            print(f\"  Tokens: Input={row.get('tokens_in', 0)}, Output={row.get('tokens_out', 0)}\")\n",
    "            print(f\"  Finish Reason: {row.get('finish_reason', 'N/A')}\")\n",
    "            \n",
    "            # If the classifier got it wrong, print debugging info\n",
    "            if not is_correct:\n",
    "                print(f\"\\n  üîç DEBUGGING INCORRECT CLASSIFICATION:\")\n",
    "                print(f\"     Expected: '{row.get('ground_truth', 'N/A')}'\")\n",
    "                print(f\"     Got: '{row.get('response', 'N/A')}'\")\n",
    "                print(f\"     Raw Response: {repr(row.get('response', 'N/A'))}\")\n",
    "                print(f\"     All Row Data:\")\n",
    "                for key, value in row.items():\n",
    "                    if key not in ['query', 'ground_truth', 'response', 'tokens_in', 'tokens_out', 'finish_reason']:\n",
    "                        print(f\"       {key}: {value}\")\n",
    "        \n",
    "        # Calculate overall accuracy\n",
    "        total_cases = len(improved_run_details.get('content', []))\n",
    "        correct_cases = sum(1 for row in improved_run_details.get('content', []) \n",
    "                          if row.get('response', '').strip().lower() == row.get('ground_truth', '').strip().lower())\n",
    "        accuracy = (correct_cases / total_cases * 100) if total_cases > 0 else 0\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìà CLASSIFICATION ACCURACY: {correct_cases}/{total_cases} = {accuracy:.1f}%\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Per-class breakdown\n",
    "        class_stats = {}\n",
    "        for row in improved_run_details.get('content', []):\n",
    "            truth = row.get('ground_truth', '').strip()\n",
    "            pred = row.get('response', '').strip()\n",
    "            \n",
    "            if truth not in class_stats:\n",
    "                class_stats[truth] = {'total': 0, 'correct': 0}\n",
    "            \n",
    "            class_stats[truth]['total'] += 1\n",
    "            if pred.lower() == truth.lower():\n",
    "                class_stats[truth]['correct'] += 1\n",
    "        \n",
    "        print(\"\\nüìä Per-Class Performance:\")\n",
    "        for class_name, stats in class_stats.items():\n",
    "            class_accuracy = (stats['correct'] / stats['total'] * 100) if stats['total'] > 0 else 0\n",
    "            print(f\"   {class_name}: {stats['correct']}/{stats['total']} = {class_accuracy:.1f}%\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running improved experiment: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d5976",
   "metadata": {},
   "source": [
    "## Step 7: Production-Ready LLM-as-a-Judge System\n",
    "\n",
    "Now let's create a production-ready system. We'll build a new experiment with a larger, realistic dataset (no ground truth labels), and use Arato's built-in **Evaluation** system with the same judge prompts we developed. This demonstrates how to deploy LLM-as-a-Judge at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a production dataset with realistic user queries (no ground truth)\n",
    "    production_dataset_rows = [\n",
    "        {\"query\": \"Thanks for the great service today!\"},\n",
    "        {\"query\": \"This is completely unacceptable behavior.\"},\n",
    "        {\"query\": \"Can you help me with my account settings?\"},\n",
    "        {\"query\": \"You people are absolutely clueless about customer service.\"},\n",
    "        {\"query\": \"I love using this platform, it's so intuitive.\"},\n",
    "        {\"query\": \"Your support team is a bunch of incompetent fools.\"},\n",
    "        {\"query\": \"Could you please explain how this feature works?\"},\n",
    "        {\"query\": \"I'm going to report you to management for this treatment.\"},\n",
    "        {\"query\": \"This app has made my workflow so much easier.\"},\n",
    "        {\"query\": \"What a waste of time and money this turned out to be.\"},\n",
    "        {\"query\": \"The documentation is very helpful and clear.\"},\n",
    "        {\"query\": \"I demand to speak to someone who actually knows what they're doing.\"},\n",
    "        {\"query\": \"Great job on the recent updates, they work perfectly.\"},\n",
    "        {\"query\": \"This is the worst customer experience I've ever had.\"},\n",
    "        {\"query\": \"How do I reset my password?\"},\n",
    "        {\"query\": \"You should be ashamed of releasing such garbage software.\"},\n",
    "        {\"query\": \"The new interface looks clean and professional.\"},\n",
    "        {\"query\": \"I'm canceling my subscription because of your terrible service.\"},\n",
    "        {\"query\": \"Thank you for the quick response to my ticket.\"},\n",
    "        {\"query\": \"I knew it, I should have never trusted a Martian to do this job!. You are a bunch of idiots. All aliens must be destroyed!\"},\n",
    "    ]\n",
    "\n",
    "    # Create the production dataset\n",
    "    production_dataset = client.notebooks.datasets.create(\n",
    "        notebook_id=notebook_id,\n",
    "        name=\"Production User Queries\",\n",
    "        description=\"Real-world user queries for toxicity evaluation (no ground truth labels)\",\n",
    "        content=production_dataset_rows\n",
    "    )\n",
    "    \n",
    "    production_dataset_id = production_dataset['id']\n",
    "    \n",
    "    print(\"‚úÖ Production dataset created successfully!\")\n",
    "    print(f\"   ID: {production_dataset_id}\")\n",
    "    print(f\"   Name: {production_dataset['name']}\")\n",
    "    print(f\"   Rows: {len(production_dataset['content'])}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating production dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a new production experiment with a neutral prompt (not a judge itself)\n",
    "    production_prompt_config = {\n",
    "        \"model_id\": \"gpt-5-nano\",\n",
    "        \"vendor_id\": \"openai\",\n",
    "        \"prompt_template\": (\n",
    "            \"You are a helpful assistant responding to user queries. \"\n",
    "            \"Provide a brief, professional response to the following query.\\n\\n\"\n",
    "            \"User Query: {{query}}\"\n",
    "        ),\n",
    "        \"model_parameters\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create the production experiment\n",
    "    production_experiment = client.notebooks.experiments.create(\n",
    "        notebook_id=notebook_id,\n",
    "        name=\"Production Assistant\",\n",
    "        description=\"Production experiment that generates responses to user queries\",\n",
    "        prompt_config=production_prompt_config,\n",
    "        dataset_id=production_dataset_id,\n",
    "        color_index=4\n",
    "    )\n",
    "    \n",
    "    production_experiment_id = production_experiment['id']\n",
    "    \n",
    "    print(\"‚úÖ Production experiment created successfully!\")\n",
    "    print(f\"   ID: {production_experiment_id}\")\n",
    "    print(f\"   Name: {production_experiment['name']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating production experiment: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce73e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create Binary Evaluation using our original judge prompt\n",
    "    binary_eval = client.notebooks.experiments.evals.create(\n",
    "        notebook_id=notebook_id,\n",
    "        experiment_id=production_experiment_id,\n",
    "        name=\"Binary Toxicity Judge\",\n",
    "        eval_type=\"Binary\",\n",
    "        context=\"query\",  # Evaluate the input query, not the response\n",
    "        fail_on_positive=True,  # Fail if toxic content is detected\n",
    "        prompt=(\n",
    "            \"You are a toxicity and bias judge. Analyze the following user query and determine if it is toxic. \"\n",
    "            \"A query is considered toxic if it contains hate speech, harassment, or promotes violence. \"\n",
    "            \"Respond with only 'yes' or 'no'.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Binary evaluation created successfully!\")\n",
    "    print(f\"   Name: {binary_eval['name']}\")\n",
    "    print(f\"   Type: {binary_eval['eval_type']}\")\n",
    "    \n",
    "    # Create Classification Evaluation using our improved judge prompt\n",
    "    classification_eval = client.notebooks.experiments.evals.create(\n",
    "        notebook_id=notebook_id,\n",
    "        experiment_id=production_experiment_id,\n",
    "        name=\"Toxicity Level Classifier\",\n",
    "        eval_type=\"Classification\",\n",
    "        context=\"query\",  # Evaluate the input query, not the response\n",
    "        prompt=(\n",
    "            \"You are an advanced toxicity classifier. Analyze the following user query and classify its toxicity level.\\n\\n\"\n",
    "            \"Classification Guidelines:\\n\"\n",
    "            \"- **Safe**: Polite, constructive, or neutral content with no harmful intent\\n\"\n",
    "            \"- **Moderate**: Rude, dismissive, or mildly offensive content that could be hurtful but isn't severely toxic\\n\"\n",
    "            \"- **High**: Any form or Level of Hate speech, direct threats, harassment, or content promoting violence\\n\\n\"\n",
    "            \"Examples:\\n\"\n",
    "            \"- 'Thank you for your help' ‚Üí Safe\\n\"\n",
    "            \"- 'That's a stupid idea' ‚Üí Moderate\\n\"\n",
    "            \"- 'I'm going to hurt you' ‚Üí High\\n\\n\"\n",
    "            \"Respond with only one word: Safe, Moderate, or High.\"\n",
    "        ),\n",
    "        classes=[\n",
    "            {\"title\": \"Safe\", \"is_pass\": True, \"color\": \"green\"},\n",
    "            {\"title\": \"Moderate\", \"is_pass\": False, \"color\": \"yellow\"},\n",
    "            {\"title\": \"High\", \"is_pass\": False, \"color\": \"red\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Classification evaluation created successfully!\")\n",
    "    print(f\"   Name: {classification_eval['name']}\")\n",
    "    print(f\"   Type: {classification_eval['eval_type']}\")\n",
    "    print(\"   Classes: Safe (pass), Moderate (fail), High (fail)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating evaluations: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if we have an OpenAI API key\n",
    "    openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not openai_key:\n",
    "        print(\"‚ö†Ô∏è  Warning: OPENAI_API_KEY not set. Cannot run production experiment.\")\n",
    "        print(\"   Please set your OpenAI API key in the .env file to continue.\")\n",
    "    else:\n",
    "        # Create and execute the production run\n",
    "        production_run = client.notebooks.experiments.runs.create(\n",
    "            notebook_id=notebook_id,\n",
    "            experiment_id=production_experiment_id,\n",
    "            api_keys={\"openai_api_key\": openai_key}\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Production run created and initiated successfully!\")\n",
    "        print(f\"   Run ID: {production_run['id']}\")\n",
    "        print(f\"   Status: {production_run['status']}\")\n",
    "        print(f\"   Run Number: {production_run['run_number']}\")\n",
    "        print(f\"   URL: https://dev.arato.io/flow/{notebook_id}/notebook\")\n",
    "        \n",
    "        production_run_id = production_run['id']\n",
    "        \n",
    "        # Poll for run completion\n",
    "        print(\"\\nüîÑ Waiting for production run to complete...\")\n",
    "        while True:\n",
    "            production_run_details = client.notebooks.experiments.runs.retrieve(\n",
    "                notebook_id=notebook_id,\n",
    "                experiment_id=production_experiment_id,\n",
    "                run_id=production_run_id\n",
    "            )\n",
    "            \n",
    "            status = production_run_details['status']\n",
    "            print(f\"   Current Status: {status}\")\n",
    "            \n",
    "            if status in ['done', 'failed']:\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)  # Poll every 5 seconds\n",
    "        \n",
    "        # Analyze the production results\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìä PRODUCTION LLM-AS-A-JUDGE RESULTS - {len(production_run_details.get('content', []))} queries\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Collect evaluation statistics\n",
    "        binary_stats = {'toxic': 0, 'safe': 0}\n",
    "        classification_stats = {}  # Initialize as empty dict to capture actual results\n",
    "        flagged_queries = []\n",
    "        \n",
    "        for idx, row in enumerate(production_run_details.get('content', []), 1):\n",
    "            query = row.get('query', 'N/A')\n",
    "            response = row.get('response', 'N/A')\n",
    "            \n",
    "            print(f\"\\n{'‚îÄ'*60}\")\n",
    "            print(f\"Query {idx}: \\\"{query}\\\"\")\n",
    "            print(f\"Response: \\\"{response[:100]}{'...' if len(response) > 100 else ''}\\\"\")\n",
    "            \n",
    "            # Analyze evaluations\n",
    "            if row.get('evals'):\n",
    "                binary_result = None\n",
    "                classification_result = None\n",
    "                \n",
    "                for eval_result in row['evals']:\n",
    "                    eval_type = eval_result.get('type', '')\n",
    "                    \n",
    "                    if eval_type == 'Binary':\n",
    "                        # For binary: result=1 means pass (safe), result=0 means fail (toxic)\n",
    "                        binary_result_code = eval_result.get('result', 'N/A')\n",
    "                        binary_result = 'safe' if binary_result_code == 1 else 'toxic'\n",
    "                        \n",
    "                        if binary_result in binary_stats:\n",
    "                            binary_stats[binary_result] += 1\n",
    "                        \n",
    "                        result_icon = \"üö®\" if binary_result == 'toxic' else \"‚úÖ\"\n",
    "                        print(f\"  {result_icon} Binary Judge: {binary_result} (result={binary_result_code})\")\n",
    "                        \n",
    "                        if binary_result == 'toxic':\n",
    "                            flagged_queries.append(f\"Query {idx}: {query}\")\n",
    "                    \n",
    "                    elif eval_type == 'Classification':\n",
    "                        # For classification: use 'title' for the classification level\n",
    "                        classification_level = eval_result.get('title', 'N/A')\n",
    "                        classification_result_code = eval_result.get('result', 'N/A')\n",
    "                        classification_result = classification_level\n",
    "                        \n",
    "                        # Count all classification results\n",
    "                        if classification_level and classification_level != 'N/A':\n",
    "                            if classification_level not in classification_stats:\n",
    "                                classification_stats[classification_level] = 0\n",
    "                            classification_stats[classification_level] += 1\n",
    "                        \n",
    "                        color_map = {'Safe': 'üü¢', 'Moderate': 'üü°', 'High': 'üî¥'}\n",
    "                        icon = color_map.get(classification_level, '‚ùì')\n",
    "                        print(f\"  {icon} Classification: {classification_level} (pass/fail={classification_result_code})\")\n",
    "                \n",
    "                # Flag for review if moderate or high toxicity\n",
    "                if classification_result in ['Moderate', 'High'] and f\"Query {idx}: {query}\" not in flagged_queries:\n",
    "                    flagged_queries.append(f\"Query {idx}: {query}\")\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è  No evaluations found\")\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"üìà EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"\\nüîç Binary Toxicity Detection:\")\n",
    "        total_binary = sum(binary_stats.values())\n",
    "        for category, count in binary_stats.items():\n",
    "            percentage = (count / total_binary * 100) if total_binary > 0 else 0\n",
    "            print(f\"   {category.title()}: {count}/{total_binary} ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nüìä Toxicity Level Classification:\")\n",
    "        total_classification = sum(classification_stats.values())\n",
    "        for level, count in classification_stats.items():\n",
    "            percentage = (count / total_classification * 100) if total_classification > 0 else 0\n",
    "            icon = {'Safe': 'üü¢', 'Moderate': 'üü°', 'High': 'üî¥'}.get(level, '‚ùì')\n",
    "            print(f\"   {icon} {level}: {count}/{total_classification} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Display flagged queries for review\n",
    "        if flagged_queries:\n",
    "            print(f\"\\nüö® FLAGGED QUERIES FOR REVIEW ({len(flagged_queries)} total):\")\n",
    "            print(\"‚îÄ\" * 60)\n",
    "            for flagged_query in flagged_queries:\n",
    "                print(f\"   ‚Ä¢ {flagged_query}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ No queries flagged for review!\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running production experiment: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
