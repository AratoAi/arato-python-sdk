{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0d990a",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge: Evaluating AI Systems at Scale\n",
    "\n",
    "**Demo for: \"Scalable LLM Evaluations in Practice\"**\n",
    "\n",
    "---\n",
    "\n",
    "## What This Demo Covers\n",
    "\n",
    "This notebook demonstrates the **complete evaluation lifecycle** using the Arato SDK:\n",
    "\n",
    "1. **Define Evaluation Goals** ‚Äî What are we measuring?\n",
    "2. **Design Criteria & Datasets** ‚Äî Prepare test data and scoring rubrics\n",
    "3. **Choose Judges & Prompts** ‚Äî Configure AI models to evaluate outputs\n",
    "4. **Run Evaluations** ‚Äî Execute experiments programmatically\n",
    "5. **Meta-Evaluate the Judge** ‚Äî Verify judge consistency and reliability\n",
    "6. **Run the LLM Judge on real data** ‚Äî Use the judge evaluation criteria and measure results\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Experiments**: Define prompts and models to generate AI responses\n",
    "- **Datasets**: Structured input data for testing AI behavior\n",
    "- **Evaluations**: Automated scoring using Binary, Numeric, and Classification judges\n",
    "- **Runs**: Execute experiments at scale and collect evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e72ddb",
   "metadata": {},
   "source": [
    "## 1. Import Required Packages\n",
    "\n",
    "First, let's import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from arato_client import AratoClient, AsyncAratoClient, NotFoundError\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817efeba",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Arato Client & Create a Notebook\n",
    "\n",
    "With our environment set up, we can now initialize the `AratoClient`. We'll also create a new **Notebook** to house our evaluation experiment. Notebooks are top-level containers for organizing work in Arato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for required environment variables\n",
    "arato_api_key = os.environ.get(\"ARATO_API_KEY\")\n",
    "if not arato_api_key:\n",
    "    print(\"‚ùå Error: ARATO_API_KEY not found in environment variables.\")\n",
    "    print(\"   Please set your Arato API key in the .env file to continue.\")\n",
    "    print(\"   Example: ARATO_API_KEY=your_api_key_here\")\n",
    "    raise ValueError(\"ARATO_API_KEY environment variable is required\")\n",
    "\n",
    "print(\"‚úÖ ARATO_API_KEY found in environment variables\")\n",
    "\n",
    "# Initialize the client\n",
    "# The API key is automatically loaded from the ARATO_API_KEY environment variable\n",
    "client = AratoClient()\n",
    "\n",
    "# Create a unique name for our demo notebook\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "notebook_name = f\"LLM-as-a-Judge Demo - {timestamp}\"\n",
    "\n",
    "# Create the notebook\n",
    "try:\n",
    "    notebook = client.notebooks.create(\n",
    "        name=notebook_name,\n",
    "        description=\"A notebook for demonstrating LLM-as-a-Judge evaluations.\",\n",
    "        tags=[\"llm-as-a-judge\", \"demo\", \"evaluation\"]\n",
    "    )\n",
    "    \n",
    "    # Store the notebook_id for later use\n",
    "    notebook_id = notebook['id']\n",
    "    \n",
    "    print(\"‚úÖ Notebook created successfully!\")\n",
    "    print(f\"   ID: {notebook_id}\")\n",
    "    print(f\"   Name: {notebook['name']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating notebook: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79ecb6",
   "metadata": {},
   "source": [
    "## Step 3: Configure the Experiment (The \"Judge\")\n",
    "\n",
    "Every new notebook in Arato includes a default experiment. We'll use this experiment to define our **LLM-as-a-Judge**.\n",
    "\n",
    "Our goal is to create a \"Judge\" that evaluates user queries for toxicity. We will configure its prompt to analyze a `query` and determine if it is toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d859e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Extract the experiment href from the created notebook object\n",
    "    experiment_href = notebook['_links']['experiments']['href']\n",
    "    \n",
    "    # The experiment ID is the last part of the href path\n",
    "    experiment_id = experiment_href.split('/')[-1]\n",
    "    \n",
    "    print(f\"‚úÖ Using default experiment with ID: {experiment_id}\")\n",
    "\n",
    "    # Define the LLM-as-a-Judge prompt\n",
    "    judge_prompt_config = {\n",
    "        \"model_id\": \"gpt-4o-mini\",\n",
    "        \"vendor_id\": \"openai\",\n",
    "        \"prompt_template\": (\n",
    "            \"You are a toxicity and bias judge. Analyze the following user query and determine if it is toxic. \"\n",
    "            \"A query is considered toxic if it contains hate speech, harassment, or promotes violence. \"\n",
    "            \"Respond with only 'yes' or 'no'.\\n\\n\"\n",
    "            \"User Query: {{query}}\"\n",
    "        ),\n",
    "        \"model_parameters\": {\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 500\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Update the experiment to be our \"Judge\"\n",
    "    updated_experiment = client.notebooks.experiments.update(\n",
    "        notebook_id=notebook_id,\n",
    "        experiment_id=experiment_id,\n",
    "        name=\"Toxicity & Bias Judge\",\n",
    "        prompt_config=judge_prompt_config\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error configuring experiment: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256dc7ab",
   "metadata": {},
   "source": [
    "## Step 4: Create a Dataset for Evaluation\n",
    "\n",
    "Now that our \"Judge\" is configured, we need data to evaluate. We'll create a simple **Dataset** containing a `query` column (the input for our Judge) and a `ground_truth` column (the expected \"correct\" answer). This allows us to measure the Judge's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e686c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the dataset from CSV file\n",
    "    df = pd.read_csv('toxicity_test_cases.csv')\n",
    "    # Convert DataFrame to list of dicts for Arato\n",
    "    dataset_rows = df.to_dict('records')\n",
    "\n",
    "    # Create the dataset in Arato\n",
    "    dataset = client.notebooks.datasets.create(\n",
    "        notebook_id=notebook_id,\n",
    "        name=\"Toxicity Test Cases\",\n",
    "        description=\"A small, human-labeled dataset to test the Toxicity & Bias Judge.\",\n",
    "        content=dataset_rows\n",
    "    )\n",
    "    \n",
    "    # Store the dataset_id for later use\n",
    "    dataset_id = dataset['id']\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset created successfully in Arato!\")\n",
    "    print(f\"   ID: {dataset_id}\")\n",
    "    print(f\"   Name: {dataset['name']}\")\n",
    "    print(f\"   Rows: {len(dataset['content'])}\")\n",
    "    display(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: toxicity_test_cases.csv not found\")\n",
    "    print(\"   Please make sure the CSV file is in the same directory as this notebook\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b0021",
   "metadata": {},
   "source": [
    "## Step 5: Run our LLM Judge Against the Test Dataset\n",
    "\n",
    "Now we need to run and compare the LLM output to our ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0fe813",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # First, update the experiment to use our dataset\n",
    "    updated_experiment = client.notebooks.experiments.update(\n",
    "        notebook_id=notebook_id,\n",
    "        experiment_id=experiment_id,\n",
    "        dataset_id=dataset_id\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Experiment updated with dataset!\")\n",
    "    print(f\"   Dataset ID: {dataset_id}\")\n",
    "    \n",
    "    # Check if we have an OpenAI API key\n",
    "    openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not openai_key:\n",
    "        print(\"\\n‚ö†Ô∏è  Warning: OPENAI_API_KEY not set. Cannot run experiment.\")\n",
    "        print(\"   Please set your OpenAI API key in the .env file to continue.\")\n",
    "    else:\n",
    "        # Create and execute a run\n",
    "        run = client.notebooks.experiments.runs.create(\n",
    "            notebook_id=notebook_id,\n",
    "            experiment_id=experiment_id,\n",
    "            api_keys={\"openai_api_key\": openai_key}\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Run created and initiated successfully!\")\n",
    "        print(f\"   Run ID: {run['id']}\")\n",
    "        print(f\"   Status: {run['status']}\")\n",
    "        print(f\"   Run Number: {run['run_number']}\")\n",
    "        \n",
    "        run_id = run['id']\n",
    "        \n",
    "        # Poll for run completion\n",
    "        print(\"\\nüîÑ Waiting for run to complete...\")\n",
    "        while True:\n",
    "            run_details = client.notebooks.experiments.runs.retrieve(\n",
    "                notebook_id=notebook_id,\n",
    "                experiment_id=experiment_id,\n",
    "                run_id=run_id\n",
    "            )\n",
    "            \n",
    "            status = run_details['status']\n",
    "            print(f\"   Current Status: {status}\")\n",
    "            \n",
    "            if status in ['done', 'failed']:\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)  # Poll every 5 seconds\n",
    "        \n",
    "        # Prepare results for table display\n",
    "        results_data = []\n",
    "        correct_cases = 0\n",
    "        total_cases = len(run_details.get('content', []))\n",
    "        \n",
    "        for idx, row in enumerate(run_details.get('content', []), 1):\n",
    "            query = row.get('query', 'N/A')\n",
    "            ground_truth = row.get('ground_truth', 'N/A')\n",
    "            judge_output = row.get('response', 'N/A')\n",
    "            \n",
    "            # Check if judge was correct\n",
    "            is_correct = judge_output.strip().lower() == ground_truth.strip().lower()\n",
    "            if is_correct:\n",
    "                correct_cases += 1\n",
    "            \n",
    "            result_icon = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "            result_text = f\"{result_icon} {'CORRECT' if is_correct else 'INCORRECT'}\"\n",
    "            \n",
    "            results_data.append({\n",
    "                'Test Case': idx,\n",
    "                'Query': query[:50] + '...' if len(query) > 50 else query,\n",
    "                'Ground Truth': ground_truth,\n",
    "                'Judge Output': judge_output,\n",
    "                'Result': result_text,\n",
    "                'Tokens In': row.get('tokens_in', 0),\n",
    "                'Tokens Out': row.get('tokens_out', 0)\n",
    "            })\n",
    "        \n",
    "        # Display results as formatted table\n",
    "        print(f\"\\nüìä TOXICITY JUDGE RESULTS - {total_cases} test cases\\n\")\n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        display(results_df)\n",
    "        \n",
    "        # Calculate and display accuracy\n",
    "        accuracy = (correct_cases / total_cases * 100) if total_cases > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüìà OVERALL ACCURACY SUMMARY\\n\")\n",
    "        accuracy_df = pd.DataFrame([{\n",
    "            'Total Cases': total_cases,\n",
    "            'Correct': correct_cases,\n",
    "            'Incorrect': total_cases - correct_cases,\n",
    "            'Accuracy': f\"{accuracy:.1f}%\"\n",
    "        }])\n",
    "        display(accuracy_df)\n",
    "        \n",
    "        # Show incorrect predictions for debugging\n",
    "        incorrect_data = []\n",
    "        for idx, row in enumerate(run_details.get('content', []), 1):\n",
    "            judge_output = row.get('response', '').strip().lower()\n",
    "            ground_truth = row.get('ground_truth', '').strip().lower()\n",
    "            \n",
    "            if judge_output != ground_truth:\n",
    "                incorrect_data.append({\n",
    "                    'Test Case': idx,\n",
    "                    'Query': row.get('query', 'N/A'),\n",
    "                    'Expected': row.get('ground_truth', 'N/A'),\n",
    "                    'Got': row.get('response', 'N/A'),\n",
    "                    'Finish Reason': row.get('finish_reason', 'N/A')\n",
    "                })\n",
    "        \n",
    "        if incorrect_data:\n",
    "            print(f\"\\nüîç INCORRECT PREDICTIONS ({len(incorrect_data)} cases):\\n\")\n",
    "            incorrect_df = pd.DataFrame(incorrect_data)\n",
    "            display(incorrect_df)\n",
    "        else:\n",
    "            print(\"\\n‚úÖ All predictions were correct!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running experiment: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08babab1",
   "metadata": {},
   "source": [
    "## Step 6: Improve the Judge with Classification Levels\n",
    "\n",
    "Based on the results, our binary judge might be missing borderline cases. Let's create an improved judge that uses **three toxicity levels**: Safe, Moderate, and High. This will help us catch more nuanced forms of toxicity that aren't overtly hateful but still problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f480d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a new, improved experiment with classification-based judging\n",
    "    improved_judge_prompt_config = {\n",
    "        \"model_id\": \"gpt-4o-mini\",\n",
    "        \"vendor_id\": \"openai\",\n",
    "        \"prompt_template\": (\n",
    "            \"You are an advanced toxicity classifier. Analyze the following user query and classify its toxicity level.\\n\\n\"\n",
    "            \"Classification Guidelines:\\n\"\n",
    "            \"- **Safe**: Polite, constructive, or neutral content with no harmful intent\\n\"\n",
    "            \"- **Moderate**: Rude, dismissive, or mildly offensive content that could be hurtful but isn't severely toxic\\n\"\n",
    "            \"- **High**: Any form or Level of Hate speech, direct threats, harassment, or content promoting violence\\n\\n\"\n",
    "            \"Examples:\\n\"\n",
    "            \"- 'Thank you for your help' ‚Üí Safe\\n\"\n",
    "            \"- 'That's a stupid idea' ‚Üí Moderate\\n\"\n",
    "            \"- 'I'm going to hurt you' ‚Üí High\\n\\n\"\n",
    "            \"Respond with only one word: Safe, Moderate, or High.\\n\\n\"\n",
    "            \"User Query: {{query}}\"\n",
    "        ),\n",
    "        \"model_parameters\": {\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 500\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create a new experiment for the improved judge\n",
    "    improved_experiment = client.notebooks.experiments.create(\n",
    "        notebook_id=notebook_id,\n",
    "        name=\"Advanced Toxicity Classifier\",\n",
    "        description=\"Three-level toxicity classification: Safe, Moderate, High\",\n",
    "        prompt_config=improved_judge_prompt_config,\n",
    "        color_index=3\n",
    "    )\n",
    "    \n",
    "    improved_experiment_id = improved_experiment['id']\n",
    "    \n",
    "    print(\"‚úÖ Improved experiment created successfully!\")\n",
    "    print(f\"   ID: {improved_experiment_id}\")\n",
    "    print(f\"   Name: {improved_experiment['name']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating improved experiment: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the improved dataset from CSV file\n",
    "    df = pd.read_csv('advanced_toxicity_classification.csv')\n",
    "    \n",
    "    # Display the dataset\n",
    "    print(\"üìä Advanced Toxicity Classification Dataset:\")\n",
    "    print(\"=\"*60)\n",
    "    display(df.head())\n",
    "    \n",
    "    # Show distribution of each level\n",
    "    distribution = Counter(df['ground_truth'])\n",
    "    print(f\"\\n‚úÖ Loaded {len(df)} rows\")\n",
    "    print(\"\\nüìä Dataset Distribution:\")\n",
    "    for level, count in distribution.items():\n",
    "        print(f\"   {level}: {count} cases\")\n",
    "    \n",
    "    # Convert DataFrame to list of dicts for Arato\n",
    "    improved_dataset_rows = df.to_dict('records')\n",
    "\n",
    "    # Create the improved dataset in Arato\n",
    "    improved_dataset = client.notebooks.datasets.create(\n",
    "        notebook_id=notebook_id,\n",
    "        name=\"Advanced Toxicity Classification Dataset\",\n",
    "        description=\"Multi-level toxicity dataset with Safe, Moderate, and High classifications\",\n",
    "        content=improved_dataset_rows\n",
    "    )\n",
    "    \n",
    "    improved_dataset_id = improved_dataset['id']\n",
    "    \n",
    "    print(f\"\\n‚úÖ Improved dataset created successfully in Arato!\")\n",
    "    print(f\"   ID: {improved_dataset_id}\")\n",
    "    print(f\"   Name: {improved_dataset['name']}\")\n",
    "    print(f\"   Rows: {len(improved_dataset['content'])}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: advanced_toxicity_classification.csv not found\")\n",
    "    print(\"   Please make sure the CSV file is in the same directory as this notebook\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating improved dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62825a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Update the improved experiment to use the new dataset\n",
    "    updated_improved_experiment = client.notebooks.experiments.update(\n",
    "        notebook_id=notebook_id,\n",
    "        experiment_id=improved_experiment_id,\n",
    "        dataset_id=improved_dataset_id\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Improved experiment updated with dataset!\")\n",
    "    print(f\"   Dataset ID: {improved_dataset_id}\")\n",
    "    \n",
    "    # Check if we have an OpenAI API key\n",
    "    openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not openai_key:\n",
    "        print(\"\\n‚ö†Ô∏è  Warning: OPENAI_API_KEY not set. Cannot run experiment.\")\n",
    "        print(\"   Please set your OpenAI API key in the .env file to continue.\")\n",
    "    else:\n",
    "        # Create and execute a run with the improved judge\n",
    "        improved_run = client.notebooks.experiments.runs.create(\n",
    "            notebook_id=notebook_id,\n",
    "            experiment_id=improved_experiment_id,\n",
    "            api_keys={\"openai_api_key\": openai_key}\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Improved run created and initiated successfully!\")\n",
    "        print(f\"   Run ID: {improved_run['id']}\")\n",
    "        print(f\"   Status: {improved_run['status']}\")\n",
    "        print(f\"   Run Number: {improved_run['run_number']}\")\n",
    "        \n",
    "        improved_run_id = improved_run['id']\n",
    "        \n",
    "        # Poll for run completion\n",
    "        print(\"\\nüîÑ Waiting for improved run to complete...\")\n",
    "        while True:\n",
    "            improved_run_details = client.notebooks.experiments.runs.retrieve(\n",
    "                notebook_id=notebook_id,\n",
    "                experiment_id=improved_experiment_id,\n",
    "                run_id=improved_run_id\n",
    "            )\n",
    "            \n",
    "            status = improved_run_details['status']\n",
    "            print(f\"   Current Status: {status}\")\n",
    "            \n",
    "            if status in ['done', 'failed']:\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)  # Poll every 5 seconds\n",
    "        \n",
    "        # Prepare results for table display\n",
    "        results_data = []\n",
    "        correct_cases = 0\n",
    "        total_cases = len(improved_run_details.get('content', []))\n",
    "        class_stats = {}\n",
    "        \n",
    "        for idx, row in enumerate(improved_run_details.get('content', []), 1):\n",
    "            query = row.get('query', 'N/A')\n",
    "            ground_truth = row.get('ground_truth', 'N/A').strip()\n",
    "            classifier_output = row.get('response', 'N/A').strip()\n",
    "            \n",
    "            # Check if classifier was correct\n",
    "            is_correct = classifier_output.lower() == ground_truth.lower()\n",
    "            if is_correct:\n",
    "                correct_cases += 1\n",
    "            \n",
    "            result_icon = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "            result_text = f\"{result_icon} {'CORRECT' if is_correct else 'INCORRECT'}\"\n",
    "            \n",
    "            # Update class statistics\n",
    "            if ground_truth not in class_stats:\n",
    "                class_stats[ground_truth] = {'total': 0, 'correct': 0}\n",
    "            class_stats[ground_truth]['total'] += 1\n",
    "            if is_correct:\n",
    "                class_stats[ground_truth]['correct'] += 1\n",
    "            \n",
    "            results_data.append({\n",
    "                'Test Case': idx,\n",
    "                'Query': query[:50] + '...' if len(query) > 50 else query,\n",
    "                'Ground Truth': ground_truth,\n",
    "                'Classifier Output': classifier_output,\n",
    "                'Result': result_text,\n",
    "                'Tokens In': row.get('tokens_in', 0),\n",
    "                'Tokens Out': row.get('tokens_out', 0)\n",
    "            })\n",
    "        \n",
    "        # Display results as formatted table\n",
    "        print(f\"\\nüìä ADVANCED TOXICITY CLASSIFIER RESULTS - {total_cases} test cases\\n\")\n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        display(results_df)\n",
    "        \n",
    "        # Calculate and display overall accuracy\n",
    "        accuracy = (correct_cases / total_cases * 100) if total_cases > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüìà OVERALL ACCURACY SUMMARY\\n\")\n",
    "        accuracy_df = pd.DataFrame([{\n",
    "            'Total Cases': total_cases,\n",
    "            'Correct': correct_cases,\n",
    "            'Incorrect': total_cases - correct_cases,\n",
    "            'Accuracy': f\"{accuracy:.1f}%\"\n",
    "        }])\n",
    "        display(accuracy_df)\n",
    "        \n",
    "        # Display per-class performance\n",
    "        print(f\"\\nüìä PER-CLASS PERFORMANCE\\n\")\n",
    "        class_performance_data = []\n",
    "        for class_name in ['Safe', 'Moderate', 'High']:\n",
    "            if class_name in class_stats:\n",
    "                stats = class_stats[class_name]\n",
    "                class_accuracy = (stats['correct'] / stats['total'] * 100) if stats['total'] > 0 else 0\n",
    "                class_performance_data.append({\n",
    "                    'Class': class_name,\n",
    "                    'Total': stats['total'],\n",
    "                    'Correct': stats['correct'],\n",
    "                    'Incorrect': stats['total'] - stats['correct'],\n",
    "                    'Accuracy': f\"{class_accuracy:.1f}%\"\n",
    "                })\n",
    "        \n",
    "        if class_performance_data:\n",
    "            class_performance_df = pd.DataFrame(class_performance_data)\n",
    "            display(class_performance_df)\n",
    "        \n",
    "        # Show incorrect predictions for debugging\n",
    "        incorrect_data = []\n",
    "        for idx, row in enumerate(improved_run_details.get('content', []), 1):\n",
    "            classifier_output = row.get('response', '').strip().lower()\n",
    "            ground_truth = row.get('ground_truth', '').strip().lower()\n",
    "            \n",
    "            if classifier_output != ground_truth:\n",
    "                incorrect_data.append({\n",
    "                    'Test Case': idx,\n",
    "                    'Query': row.get('query', 'N/A'),\n",
    "                    'Expected': row.get('ground_truth', 'N/A'),\n",
    "                    'Got': row.get('response', 'N/A'),\n",
    "                    'Finish Reason': row.get('finish_reason', 'N/A')\n",
    "                })\n",
    "        \n",
    "        if incorrect_data:\n",
    "            print(f\"\\nüîç INCORRECT CLASSIFICATIONS ({len(incorrect_data)} cases):\\n\")\n",
    "            incorrect_df = pd.DataFrame(incorrect_data)\n",
    "            display(incorrect_df)\n",
    "        else:\n",
    "            print(\"\\n‚úÖ All classifications were correct!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running improved experiment: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d5976",
   "metadata": {},
   "source": [
    "## Step 7: Production-Ready LLM-as-a-Judge System\n",
    "\n",
    "Now let's create a production-ready system. We'll build a new experiment with a larger, realistic dataset (no ground truth labels), and use Arato's built-in **Evaluation** system with the same judge prompts we developed. This demonstrates how to deploy LLM-as-a-Judge at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the production dataset from CSV file\n",
    "    df = pd.read_csv('production_user_queries.csv')\n",
    "    \n",
    "    # Display the dataset\n",
    "    print(\"üìä Production User Queries Dataset:\")\n",
    "    print(\"=\"*60)\n",
    "    display(df.head(10))\n",
    "    \n",
    "    print(f\"\\n‚úÖ Loaded {len(df)} production queries\")\n",
    "    \n",
    "    # Convert DataFrame to list of dicts for Arato\n",
    "    production_dataset_rows = df.to_dict('records')\n",
    "\n",
    "    # Create the production dataset\n",
    "    production_dataset = client.notebooks.datasets.create(\n",
    "        notebook_id=notebook_id,\n",
    "        name=\"Production User Queries\",\n",
    "        description=\"Real-world user queries for toxicity evaluation (no ground truth labels)\",\n",
    "        content=production_dataset_rows\n",
    "    )\n",
    "    \n",
    "    production_dataset_id = production_dataset['id']\n",
    "    \n",
    "    print(f\"\\n‚úÖ Production dataset created successfully in Arato!\")\n",
    "    print(f\"   ID: {production_dataset_id}\")\n",
    "    print(f\"   Name: {production_dataset['name']}\")\n",
    "    print(f\"   Rows: {len(production_dataset['content'])}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: production_user_queries.csv not found\")\n",
    "    print(\"   Please make sure the CSV file is in the same directory as this notebook\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating production dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a new production experiment with a neutral prompt (not a judge itself)\n",
    "    production_prompt_config = {\n",
    "        \"model_id\": \"gpt-5-nano\",\n",
    "        \"vendor_id\": \"openai\",\n",
    "        \"prompt_template\": (\n",
    "            \"You are a helpful assistant responding to user queries. \"\n",
    "            \"Provide a brief, professional response to the following query.\\n\\n\"\n",
    "            \"User Query: {{query}}\"\n",
    "        ),\n",
    "        \"model_parameters\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create the production experiment\n",
    "    production_experiment = client.notebooks.experiments.create(\n",
    "        notebook_id=notebook_id,\n",
    "        name=\"Production Assistant\",\n",
    "        description=\"Production experiment that generates responses to user queries\",\n",
    "        prompt_config=production_prompt_config,\n",
    "        dataset_id=production_dataset_id,\n",
    "        color_index=4\n",
    "    )\n",
    "    \n",
    "    production_experiment_id = production_experiment['id']\n",
    "    \n",
    "    print(\"‚úÖ Production experiment created successfully!\")\n",
    "    print(f\"   ID: {production_experiment_id}\")\n",
    "    print(f\"   Name: {production_experiment['name']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating production experiment: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce73e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create Binary Evaluation using our original judge prompt\n",
    "    binary_eval = client.notebooks.experiments.evals.create(\n",
    "        notebook_id=notebook_id,\n",
    "        experiment_id=production_experiment_id,\n",
    "        name=\"Binary Toxicity Judge\",\n",
    "        eval_type=\"Binary\",\n",
    "        context=\"query\",  # Evaluate the input query, not the response\n",
    "        fail_on_positive=True,  # Fail if toxic content is detected\n",
    "        prompt=(\n",
    "            \"You are a toxicity and bias judge. Analyze the following user query and determine if it is toxic. \"\n",
    "            \"A query is considered toxic if it contains hate speech, harassment, or promotes violence. \"\n",
    "            \"Respond with only 'yes' or 'no'.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Binary evaluation created successfully!\")\n",
    "    print(f\"   Name: {binary_eval['name']}\")\n",
    "    print(f\"   Type: {binary_eval['eval_type']}\")\n",
    "    \n",
    "    # Create Classification Evaluation using our improved judge prompt\n",
    "    classification_eval = client.notebooks.experiments.evals.create(\n",
    "        notebook_id=notebook_id,\n",
    "        experiment_id=production_experiment_id,\n",
    "        name=\"Toxicity Level Classifier\",\n",
    "        eval_type=\"Classification\",\n",
    "        context=\"query\",  # Evaluate the input query, not the response\n",
    "        prompt=(\n",
    "            \"You are an advanced toxicity classifier. Analyze the following user query and classify its toxicity level.\\n\\n\"\n",
    "            \"Classification Guidelines:\\n\"\n",
    "            \"- **Safe**: Polite, constructive, or neutral content with no harmful intent\\n\"\n",
    "            \"- **Moderate**: Rude, dismissive, or mildly offensive content that could be hurtful but isn't severely toxic\\n\"\n",
    "            \"- **High**: Any form or Level of Hate speech, direct threats, harassment, or content promoting violence\\n\\n\"\n",
    "            \"Examples:\\n\"\n",
    "            \"- 'Thank you for your help' ‚Üí Safe\\n\"\n",
    "            \"- 'That's a stupid idea' ‚Üí Moderate\\n\"\n",
    "            \"- 'I'm going to hurt you' ‚Üí High\\n\\n\"\n",
    "            \"Respond with only one word: Safe, Moderate, or High.\"\n",
    "        ),\n",
    "        classes=[\n",
    "            {\"title\": \"Safe\", \"is_pass\": True, \"color\": \"green\"},\n",
    "            {\"title\": \"Moderate\", \"is_pass\": False, \"color\": \"yellow\"},\n",
    "            {\"title\": \"High\", \"is_pass\": False, \"color\": \"red\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Classification evaluation created successfully!\")\n",
    "    print(f\"   Name: {classification_eval['name']}\")\n",
    "    print(f\"   Type: {classification_eval['eval_type']}\")\n",
    "    print(\"   Classes: Safe (pass), Moderate (fail), High (fail)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating evaluations: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if we have an OpenAI API key\n",
    "    openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not openai_key:\n",
    "        print(\"‚ö†Ô∏è  Warning: OPENAI_API_KEY not set. Cannot run production experiment.\")\n",
    "        print(\"   Please set your OpenAI API key in the .env file to continue.\")\n",
    "    else:\n",
    "        # Create and execute the production run\n",
    "        production_run = client.notebooks.experiments.runs.create(\n",
    "            notebook_id=notebook_id,\n",
    "            experiment_id=production_experiment_id,\n",
    "            api_keys={\"openai_api_key\": openai_key}\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Production run created and initiated successfully!\")\n",
    "        print(f\"   Run ID: {production_run['id']}\")\n",
    "        print(f\"   Status: {production_run['status']}\")\n",
    "        print(f\"   Run Number: {production_run['run_number']}\")\n",
    "        print(f\"   URL: https://app.arato.ai/flow/{notebook_id}/notebook\")\n",
    "        \n",
    "        production_run_id = production_run['id']\n",
    "        \n",
    "        # Poll for run completion\n",
    "        print(\"\\nüîÑ Waiting for production run to complete...\")\n",
    "        while True:\n",
    "            production_run_details = client.notebooks.experiments.runs.retrieve(\n",
    "                notebook_id=notebook_id,\n",
    "                experiment_id=production_experiment_id,\n",
    "                run_id=production_run_id\n",
    "            )\n",
    "            \n",
    "            status = production_run_details['status']\n",
    "            print(f\"   Current Status: {status}\")\n",
    "            \n",
    "            if status in ['done', 'failed']:\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)  # Poll every 5 seconds\n",
    "        \n",
    "        # Prepare results for table display\n",
    "        results_data = []\n",
    "        binary_stats = {'toxic': 0, 'safe': 0}\n",
    "        classification_stats = {}\n",
    "        flagged_queries = []\n",
    "        \n",
    "        for idx, row in enumerate(production_run_details.get('content', []), 1):\n",
    "            query = row.get('query', 'N/A')\n",
    "            response = row.get('response', 'N/A')\n",
    "            \n",
    "            binary_result = None\n",
    "            classification_result = None\n",
    "            binary_icon = ''\n",
    "            classification_icon = ''\n",
    "            \n",
    "            # Analyze evaluations\n",
    "            if row.get('evals'):\n",
    "                for eval_result in row['evals']:\n",
    "                    eval_type = eval_result.get('type', '')\n",
    "                    \n",
    "                    if eval_type == 'Binary':\n",
    "                        binary_result_code = eval_result.get('result', 'N/A')\n",
    "                        binary_result = 'Safe' if binary_result_code == 1 else 'Toxic'\n",
    "                        binary_icon = '‚úÖ' if binary_result_code == 1 else 'üö®'\n",
    "                        \n",
    "                        if binary_result.lower() in binary_stats:\n",
    "                            binary_stats[binary_result.lower()] += 1\n",
    "                        \n",
    "                        if binary_result == 'Toxic':\n",
    "                            flagged_queries.append(query)\n",
    "                    \n",
    "                    elif eval_type == 'Classification':\n",
    "                        classification_result = eval_result.get('title', 'N/A')\n",
    "                        \n",
    "                        if classification_result and classification_result != 'N/A':\n",
    "                            if classification_result not in classification_stats:\n",
    "                                classification_stats[classification_result] = 0\n",
    "                            classification_stats[classification_result] += 1\n",
    "                        \n",
    "                        color_map = {'Safe': 'üü¢', 'Moderate': 'üü°', 'High': 'üî¥'}\n",
    "                        classification_icon = color_map.get(classification_result, '‚ùì')\n",
    "                        \n",
    "                        if classification_result in ['Moderate', 'High'] and query not in flagged_queries:\n",
    "                            flagged_queries.append(query)\n",
    "            \n",
    "            # Add to results data\n",
    "            results_data.append({\n",
    "                'Query': query[:60] + '...' if len(query) > 60 else query,\n",
    "                'Binary': f\"{binary_icon} {binary_result}\" if binary_result else 'N/A',\n",
    "                'Classification': f\"{classification_icon} {classification_result}\" if classification_result else 'N/A',\n",
    "                'Response': response[:50] + '...' if len(response) > 50 else response\n",
    "            })\n",
    "        \n",
    "        # Display results as formatted table\n",
    "        print(\"\\nüìä PRODUCTION LLM-AS-A-JUDGE RESULTS\\n\")\n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        display(results_df)\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\nüìà EVALUATION SUMMARY\\n\")\n",
    "        \n",
    "        # Binary statistics table\n",
    "        binary_df = pd.DataFrame([\n",
    "            {'Category': '‚úÖ Safe', 'Count': binary_stats.get('safe', 0), \n",
    "             'Percentage': f\"{(binary_stats.get('safe', 0) / len(results_data) * 100):.1f}%\"},\n",
    "            {'Category': 'üö® Toxic', 'Count': binary_stats.get('toxic', 0), \n",
    "             'Percentage': f\"{(binary_stats.get('toxic', 0) / len(results_data) * 100):.1f}%\"}\n",
    "        ])\n",
    "        print(\"üîç Binary Toxicity Detection:\")\n",
    "        display(binary_df)\n",
    "        \n",
    "        # Classification statistics table\n",
    "        if classification_stats:\n",
    "            classification_data = []\n",
    "            icon_map = {'Safe': 'üü¢', 'Moderate': 'üü°', 'High': 'üî¥'}\n",
    "            for level in ['Safe', 'Moderate', 'High']:\n",
    "                if level in classification_stats:\n",
    "                    count = classification_stats[level]\n",
    "                    classification_data.append({\n",
    "                        'Level': f\"{icon_map.get(level, '‚ùì')} {level}\",\n",
    "                        'Count': count,\n",
    "                        'Percentage': f\"{(count / len(results_data) * 100):.1f}%\"\n",
    "                    })\n",
    "            \n",
    "            classification_df = pd.DataFrame(classification_data)\n",
    "            print(\"\\nüìä Toxicity Level Classification:\")\n",
    "            display(classification_df)\n",
    "        \n",
    "        # Flagged queries table\n",
    "        if flagged_queries:\n",
    "            print(f\"\\nüö® FLAGGED QUERIES FOR REVIEW ({len(flagged_queries)} total):\")\n",
    "            flagged_df = pd.DataFrame({\n",
    "                'Flagged Query': flagged_queries\n",
    "            })\n",
    "            display(flagged_df)\n",
    "        else:\n",
    "            print(\"\\n‚úÖ No queries flagged for review!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running production experiment: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
